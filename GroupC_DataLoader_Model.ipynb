{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "def cuda(obj):\n",
    "    if USE_CUDA:\n",
    "        if isinstance(obj, tuple):\n",
    "            return tuple(cuda(o) for o in obj)\n",
    "        elif isinstance(obj, list):\n",
    "            return list(cuda(o) for o in obj)\n",
    "        elif hasattr(obj, 'cuda'):\n",
    "            return obj.cuda()\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataset class to pull image, label, and file name \n",
    "class DLDataset(Dataset):\n",
    "    # normalize to mean/std of image array values\n",
    "    def __init__(self, csv_file, dna_dir, chip_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (csv): csv file with annotations.\n",
    "            dna_dir (string): Directory with all the DNA data files.\n",
    "            chip_dir (string): Directory with all the ChIP data files.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.full_data = pd.read_csv(csv_file)\n",
    "        self.dna_dir = dna_dir\n",
    "        self.chip_dir = chip_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.full_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dna_file = os.path.join(self.dna_dir,\n",
    "                                self.full_data.iloc[idx][\"DNAFileSource\"])\n",
    "        chip_file = os.path.join(self.chip_dir,\n",
    "                                self.full_data.iloc[idx][\"ChIPFileSource\"])\n",
    "        dna_data = np.load(dna_file)\n",
    "        chip_data = np.load(chip_file)\n",
    "        # change chip data to 4x200\n",
    "        mod_chip_data = np.repeat(chip_data[\"x\"],4,axis=0)\n",
    "        # add dim to make 1x4x200\n",
    "        dna_data = np.expand_dims(dna_data,axis=0)\n",
    "        mod_chip_data = np.expand_dims(mod_chip_data,axis=0)\n",
    "        # input data will be nx4x200\n",
    "        try:\n",
    "            input_data = np.concatenate((dna_data,mod_chip_data))\n",
    "        except:\n",
    "            print(\"error //\")\n",
    "            print(\"dna:\",dna_data.shape)\n",
    "            print(\"mod_chip_data:\",mod_chip_data)\n",
    "            print(dna_file)\n",
    "            print(chip_file)\n",
    "    \n",
    "        label = self.full_data.iloc[idx][\"label\"]\n",
    "        \n",
    "        sample = {'data': input_data, 'label': label}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "BATCH_SIZE = 20\n",
    "test_batch_size = 20\n",
    "training_epoches = 20\n",
    "\n",
    "testingData = DLDataset(csv_file='masterDataLoadingCSV.csv',\n",
    "                        dna_dir=\"/Users/jason/PycharmProjects/Tsirigos/deeplearning/DNA\",\n",
    "                        chip_dir=\"/Users/jason/PycharmProjects/Tsirigos/deeplearning/chip_npz\"\n",
    "                       )\n",
    "testing_loader = DataLoader(testingData, batch_size=BATCH_SIZE,shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2, 4, 200])\n",
      "torch.Size([20])\n",
      "\n",
      "( 0 , 0 ,.,.) = \n",
      "   0   0   1  ...    1   0   0\n",
      "   0   1   0  ...    0   0   0\n",
      "   0   0   0  ...    0   1   1\n",
      "   1   0   0  ...    0   0   0\n",
      "\n",
      "( 0 , 1 ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "      ⋮  \n",
      "\n",
      "( 1 , 0 ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   1   0\n",
      "   1   1   0  ...    0   0   0\n",
      "   0   0   1  ...    1   0   1\n",
      "\n",
      "( 1 , 1 ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "      ⋮  \n",
      "\n",
      "( 2 , 0 ,.,.) = \n",
      "   0   1   0  ...    0   0   0\n",
      "   1   0   0  ...    0   0   0\n",
      "   0   0   1  ...    1   1   0\n",
      "   0   0   0  ...    0   0   1\n",
      "\n",
      "( 2 , 1 ,.,.) = \n",
      "   4   4   4  ...    1   1   1\n",
      "   4   4   4  ...    1   1   1\n",
      "   4   4   4  ...    1   1   1\n",
      "   4   4   4  ...    1   1   1\n",
      "...     \n",
      "      ⋮  \n",
      "\n",
      "(17 , 0 ,.,.) = \n",
      "   0   0   0  ...    0   1   0\n",
      "   0   0   1  ...    1   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   1   1   0  ...    0   0   1\n",
      "\n",
      "(17 , 1 ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "      ⋮  \n",
      "\n",
      "(18 , 0 ,.,.) = \n",
      "   0   1   1  ...    1   0   1\n",
      "   1   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   1   0\n",
      "\n",
      "(18 , 1 ,.,.) = \n",
      "   5   5   5  ...    0   0   0\n",
      "   5   5   5  ...    0   0   0\n",
      "   5   5   5  ...    0   0   0\n",
      "   5   5   5  ...    0   0   0\n",
      "      ⋮  \n",
      "\n",
      "(19 , 0 ,.,.) = \n",
      "   1   0   0  ...    0   1   0\n",
      "   0   0   1  ...    1   0   0\n",
      "   0   1   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   1\n",
      "\n",
      "(19 , 1 ,.,.) = \n",
      "   2   2   2  ...    0   0   0\n",
      "   2   2   2  ...    0   0   0\n",
      "   2   2   2  ...    0   0   0\n",
      "   2   2   2  ...    0   0   0\n",
      "[torch.DoubleTensor of size 20x2x4x200]\n",
      "\n",
      "\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.LongTensor of size 20]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample_batched in testing_loader:\n",
    "    # get the inputs\n",
    "    inputs = sample_batched['data']\n",
    "    labels = sample_batched['label']\n",
    "    print(inputs.shape)\n",
    "    print(labels.shape)\n",
    "    print(inputs)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvModel(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d (2, 20, kernel_size=(4, 6), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(1, 3), stride=(1, 3), dilation=(1, 1))\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d (20, 30, kernel_size=(1, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(1, 3), stride=(1, 3), dilation=(1, 1))\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d (30, 40, kernel_size=(1, 2), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(1, 4), stride=(1, 4), dilation=(1, 1))\n",
      "  )\n",
      "  (fc1): Linear(in_features=200, out_features=90)\n",
      "  (fc2): Linear(in_features=90, out_features=45)\n",
      "  (out): Linear(in_features=45, out_features=2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ConvModel(nn.Module):\n",
    "    def __init__(self,fc1_size=90,fc2_size=45,num_ch=2):\n",
    "        super(ConvModel, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(         # image shape (n, 4, 200)\n",
    "            nn.Conv2d(\n",
    "                in_channels=num_ch,\n",
    "                out_channels=20,\n",
    "                kernel_size=(4,6),\n",
    "                stride=(1,1),\n",
    "                padding=0\n",
    "            ),                              # output shape 20x1x195\n",
    "            nn.ReLU(),                      # activation\n",
    "            nn.MaxPool2d(kernel_size=(1,3)),    # choose max value in 2x2 area, output shape (20, 1, 65)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(20, 30, (1,3), (1,1)),    # output shape 30x1x63\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(1,3)),    # output shape (30, 1, 21)\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(30, 40, (1,2), (1,1)),    # output shape 40x1x20\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(1,4)),    # output shape (40, 1, 5)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(40*1*5, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.out = nn.Linear(fc2_size, 2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.log_softmax(self.out(x))\n",
    "        return x\n",
    "\n",
    "model = ConvModel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Steps\n",
    "# need defined: testing_loader, optimizer, model\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, sample in enumerate(testing_loader):\n",
    "        data = sample['data'].float()\n",
    "        target = sample['label']\n",
    "        if USE_CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 5000 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(testing_loader.dataset),\n",
    "                100. * batch_idx / len(testing_loader), loss.data[0]))\n",
    "        train_loss += loss.data[0]\n",
    "    train_loss /=len(testing_loader)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jason/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/112161 (0%)]\tLoss: 0.751739\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Traceback (most recent call last):\n  File \"/Users/jason/anaconda/envs/py35/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/Users/jason/anaconda/envs/py35/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 42, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-48-bcab0765cf18>\", line 26, in __getitem__\n    dna_data = np.load(dna_file)\n  File \"/Users/jason/anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/npyio.py\", line 372, in load\n    fid = open(file, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/jason/PycharmProjects/Tsirigos/deeplearning/DNA/MYCScreen_037479_DNA.npy'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-a9210a9c5e9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest_losses\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_epoches\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#     test_losses.append(test(epoch, test_loader))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-4054834754e1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py35/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py35/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Traceback (most recent call last):\n  File \"/Users/jason/anaconda/envs/py35/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/Users/jason/anaconda/envs/py35/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 42, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-48-bcab0765cf18>\", line 26, in __getitem__\n    dna_data = np.load(dna_file)\n  File \"/Users/jason/anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/npyio.py\", line 372, in load\n    fid = open(file, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/jason/PycharmProjects/Tsirigos/deeplearning/DNA/MYCScreen_037479_DNA.npy'\n"
     ]
    }
   ],
   "source": [
    "model = ConvModel()\n",
    "if USE_CUDA:\n",
    "    model.cuda()\n",
    "#Optimizor \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_losses =[]\n",
    "test_losses =[]\n",
    "for epoch in range(1, training_epoches + 1):\n",
    "    train_losses.append(train(epoch))\n",
    "#     test_losses.append(test(epoch, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_number = list(range(1, training_epoches + 1))\n",
    "plt.plot(epoch_number,train_losses, 'r' )\n",
    "plt.plot(epoch_number,test_losses, 'b' )\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
